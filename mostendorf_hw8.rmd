---
title: "Problem Set 8"
author: "Matt Ostendorf"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggpubr)
library(foreign)

```

## Introduction

Please complete the following tasks regarding the data in R. Please generate a solution document in R markdown and upload the .Rmd document and a rendered .doc, .docx, or .pdf document. Your work should be based on the data's being in the same folder as the .Rmd file. Please turn in your work on Canvas. Your solution document should have your answers to the questions and should display the requested plots. Each part is worth 10 points.

These questions were rendered in R markdown through RStudio (<https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf>, <http://rmarkdown.rstudio.com> ).

# Question 1

Let $W$ be a random variable with expected value equal to $\mu_W$ and variance equal to $\sigma^2_W$.Let $X$ be a random variable with expected value equal to $\mu_X$ and variance equal to $\sigma^2_X$. Suppose $W$ and $X$ are independent. Define the random variable $Y$ by $Y=X+W$. Recall that if $U$ and $V$ are independent random variables then $E[UV]=E[U]E[V]$. (This is true provided $Var[U]$ and $Var[V]$ are defined.)

## Question 1, part 1

(5 points)

Please compute the covariance of $X$ and $Y$, $Cov[X,Y]$, in terms of $\mu_W$, $\sigma^2_W$, $\mu_X$, and $\sigma^2_X$.

$Cov[X,Y] = E[XY] - E[X]E[Y]$

Substitute in values for Y

$Cov[X,Y] = E[X(X+W)] - E[X]E[X+W]$

$= E[X^2 + XW] - E[X](E[X]+E[W])$

$= E[X^2] + E[XW] - (E[X])^2 - E[X]E[W]$

Substitute in E[XW] = E[X]E[W]

$= Var(X) + (E[X])^2 + E[X]E[W] - (E[X])^2 - E[X]E[W]$

$=Var(X)$

$=\sigma^2_X$

$Cov[X,Y] = \sigma^2_X$

Question 1, part 2

(5 points)

Let $W$, $X$, and $Y$ be as defined above and suppose the correlation of $X$ and $Y$ equals $\rho>0$. Find the value of the ratio $\frac{\sigma^2_W}{\sigma^2_X}$ in terms of $\rho$.

$Cor[X,Y] = \rho$

$= \frac{Cov[X,Y]}{\sqrt{\sigma^2_X} \sqrt{Var(Y)}}$

substittue value of Cov[X,Y]

$= \frac{\sigma^2_X}{\sigma_X\sqrt{Var(Y)}}$

Cancel out a sigma on top

$= \frac{\sigma_X}{\sqrt{Var(Y)})}$

substitute value for Var(Y) = Var(X) + Var(W) + 2Cov[X,W]. 2Cov[X,W] is zero since they are independent

$= \frac{\sigma_X}{\sqrt{Var(X)} + \sqrt{Var(W)}}$

substitute values in for var(x) and var(w)

$= \frac{\sigma_X}{\sqrt{\sigma^2_X +\sigma^2_W}}$

square both sides

$\rho^2 = \frac{\sigma^2_X}{\sigma^2_X + \sigma^2_W}$

$\frac{1}{\rho^2} = \frac{\sigma^2_X + \sigma^2_W}{\sigma^2_X}$

split out fraction on right side

$\frac{1}{\rho^2} = \frac{\sigma^2_X}{\sigma^2_X} + \frac{\sigma^2_W}{\sigma^2_X}$

Reduce like terms

$\frac{1}{\rho^2} = 1 +\frac{\sigma^2_W}{\sigma^2_X}$

Get in terms of $\frac{\sigma^2_W}{\sigma^2_X}$

$\frac{\sigma^2_W}{\sigma^2_X} = \frac{1}{\rho^2} -1$

# Question 2

This problem set uses 2019 data primarily for Denver county accessed through IPUMS-USA, University of Minnesota, www.ipums.org ,

Steven Ruggles, Sarah Flood, Ronald Goeken, Josiah Grover, Erin Meyer, Jose Pacas and Matthew Sobek. IPUMS USA: Version 7.0 [dataset]. Minneapolis, MN: IPUMS, 2020. <https://doi.org/10.18128/D010.V10.0>

The PUMA-to-county restriction was done using MABLE, <https://mcdc.missouri.edu/applications/geocorr.html>

This problem set uses a subsample of demographic data for the Denver area.

The sample was drawn according to the values in the variable "perwt". This is a weight value provided by the US Census Bureau to correct for differences between the sampled population and the target population. It is called a sample weight or an expansion weight.

It can be thought of as the number of people in Colorado that the one observation represents in terms of demographic characteristics. For example, if you add all the weights in the original sample for all of Colorado, you will get an approximation of the population of Colorado in the sample year. If you multiply the "age" variable by "perwt" then divide by the sum of the "perwt" values, you will get an approximation of the average age in the state, whether or not the ages of the cases are present in the same proportion in the sample as in the population.

Thus a sample drawn using "perwt" as the probability will be a better approximation of the population than a simple random sample in which each case has an equal likelihood of being selected.

The category "educ"=7 corresponds to 1 year of college. The category "educ"=10 corresponds to 4 years of college.

Samples of size 25 are drawn from the responses with "educ"=7 and with "educ"=10 according to the weights in the data set and saved in "dat_7\_10.RData".

### IPUMS Data

Read in the subsample of the IPUMS data.

```{r}

load("dat_7_10.RData")

```

## Question 2, part 1

(5 points)

Are these samples consistent with Normality in the distributions of the two populations sampled? Please perform a visual check. Please perform Welch's test for the null hypothesis of equal means. Please interpret the results given the assessment of Normality.

The first plot (1 year of colege) is fairly consistent with normality however it does have an outlier in the far right. The second plot (4 years of college) has all the values fall within the acceptable range of normality. When we run Welch's test with 95% confidence interval we get a small p value (.03). Which is smaller than our alpha. This means we can reject the null hypothesis and state that these two populations have different means.

```{r}
incwage7 = dat.7.10$incwage[dat.7.10$educ==7]
incwage10 = dat.7.10$incwage[dat.7.10$educ==10]
ggqqplot(dat.7.10$incwage[dat.7.10$educ==7])
ggqqplot(dat.7.10$incwage[dat.7.10$educ==10])



```

```{r}
t.test(incwage7,incwage10)
```

## Question 2, part 2

(5 points)

The interpretation of the Mann-Whitney U test depends on whether the distributions of the two populations sampled are viewed as related by translation. Thus a standard first step before applying the Mann-Whitney U test is to assess this assumption visually or on the basis of subject matter expertise. Given the plots below, which description best describes the relationship of the samples?

-   The plots clearly indicate that distributions of the two populations sampled are related by translation.

-   The plots are somewhat consistent with the distributions of the two populations sampled being related by translation.

-   The plots clearly indicate that distributions of the two populations sampled are related by not translation.

Based on the below graphs the two populations appear to be somewhat consistent with the distribution of two populations sampled being related by translation.

```{r}
ggplot(dat.7.10,aes(x=incwage,color=factor(educ)))+geom_density()
ggplot(dat.7.10,aes(x=incwage,fill=factor(educ)))+geom_histogram(position="dodge",bins=8)
```

## Question 2, part 3

(5 points)

Please run and interpret a Mann-Whitney U test comparing "incwage" for the observations with "educ" equal to 7 and "incwage" for the observations with "educ" equal to 10. In your interpretation, please address both the case in which you treat the distributions of the two populations as related by translation and the case in which you don't make this assumption. (Note that the "wilcox.test" function will use an approximation when there are ties in the outcome values. You may treat the test with this approximation as acceptable.)

When we run the Mann-Whitney U test comparing wages from 1 year of college to 4 years of college with taking translation into account we get a p value of .008981 which is smaller than our alpha of .05. When we run the test not taking translation into account we get a p value of .008729. In both scenarios we get a a similar p value and both are smaller than our alpha of .05. This means we can reject the null hypothesis.

```{r}
wilcox.test(incwage7,incwage10, paird = FALSE, correct = TRUE)
wilcox.test(incwage7,incwage10, paird = FALSE, correct = FALSE)

```

## Question 2, part 4

(5 points)

Please run a Mann-Whitney U test comparing log(incwage) for the observations with "educ" equal to 7 and with "educ" equal to 10 and compare to the result in part a. Please explain what you observe about the two tests.

When we run the Mann-Whitney U test on the log of the wages we don't see any difference in the p values for either accounting for translation or not. This means we can still reject the null hypothesis.

```{r}
wilcox.test(log(incwage7), log(incwage10), paird = FALSE, correct = TRUE)
wilcox.test(log(incwage7), log(incwage10), paird = FALSE, correct = FALSE)
```

# Question 3

The raw data in this question is the "Pew Research Center's American Trends Panel" Wave 69 Field dates: June 16 -- June 22, 2020 Topics: Coronavirus tracking, politics, 2020 Census data and questionnaire downloaded 3/4/2021 from <https://www.pewresearch.org/politics/dataset/american-trends-panel-wave-69/>

The codebook was downloaded 3/5/2021 from <https://www.pewresearch.org/wp-content/uploads/2018/05/Codebook-and-instructions-for-working-with-ATP-data.pdf>

The Pew Research Center provides sample weights in the variable "WEIGHT_W69". These serve a similar purpose to the "perwt" variable in the IPUMS data, though these weights have the effect of readjusting the proportions of demographic groups in the sample to be approximately the proportions in the target population when the responses are viewed as representing the number of people given by the weight. The weights add up to the number of responses in the study.

The code below draws a sample from the full response set with probability based on the weight. Please use "dat.sub" in the questions below. The data frame "dat.sub" is provided with the assignment.

```{r}
# The file path will need to be adjusted for the local system's directory structure:
# dat.pew<-data.frame(read.spss("W69_Jun20/ATP W69.sav"))
# sum(dat.pew$WEIGHT_W69)
# set.seed(1234)
# sub.index<-sample(1:nrow(dat.pew),200,prob = dat.pew$WEIGHT_W69,replace=TRUE)
# dat.sub<-dat.pew[sub.index,]
# save(dat.sub,file="dat_sub.RData")
load("dat_sub.RData")
```

The code below generates a contingency table for the answers to the question "How much of a problem do you think each of the following are in the country today?" applied to the coronavirus outbreak by the age category of the respondent. The respondents who refused to supply their age or an answer to the question are omitted.

For intuition, the percent within each age range selecting each response is shown.

If you would prefer to investigate the independence of another pair of variables, you may generate your own contingency table and base your answers to part 1 and part 2 on your own table.

```{r}
t<-table(dat.sub$F_AGECAT,dat.sub$NATPROBS_b_W69)
(t<-t[1:4,1:4]) # Drop the "Refused" row and column

# percent of each row that lies in the corresponding column
round(100*t/rowSums(t),0)
```

## Question 3 part 1

(10 points)

Please use the $\chi^2$ test to test the independence of the probability distribution with the outcomes in the rows and the probability distribution with the outcomes in the columns for the table you chose. Is this an appropriate test for your table? If so, please respond to the question of whether the data are consistent with the null hypothesis that the row distribution and the column distribution are independent.

When we run the chi squared test on the data set we get a p value of .6246. This p value is greater than our alpha of .05. Because of this we can fail to reject our null hypothesis that the row distribution and column distribution are independent. However due to the data sample size we cannot say with 100% confidence that we can fail to reject the null hypothesis and the chi squared test might not be the best test for this data set.

```{r}
chisq.test(t, correct = FALSE)
```

### Question 3, part 2

(10 points)

Please use Fisher's exact test to test the independence of the probability distribution with the outcomes in the rows and the probability distribution with the outcomes in the columns for the table you chose. Is this an appropriate test for your table? If so, please respond to the question of whether the data are consistent with the null hypothesis that the row distribution and the column distribution are independent. (Setting cache=TRUE means that the code in the block will not be reevaluated on subsequent "knit" applications unless the code in the block is changed. This speeds up text editing once the calculations are in place, but shouldn't be used before than because the calculations won't be updated to reflect changes elsewhere.)

When we run the Fisher's exact test on the table we get a p value of .6345. This value is greater than our alpha of .05, therefore we can fail to reject the null hypothesis that that probability distribution in the rows and the probability distribution with the columns are independent. In the fischer's test we are able to increase the workspace which allows us to have more confidence of the results compared to the chi squared test.

```{r cache=TRUE }
 # Note that the argument "workspace" can increased, for example, "workspace=1e6"
# can be slow
cahce = TRUE
fisher.test(t, workspace = 1e6)
```
