---
title: "Problem Set 3"
author: "Matthew Ostendorf"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(HistData)
library(tidyverse)
```

# Introduction

These questions were rendered in R markdown through RStudio (<https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf>, <http://rmarkdown.rstudio.com> ).

Please generate your solutions in R markdown, without later editing, and upload both a knitted doc, docx, or pdf document in addition to the Rmd file. Please be sure that the knitted document displays the results of your calculations.

The questions in this problem set use material from the slides on parameter estimation.

# Question 1

## Context

We have seen that count $k$ of successes from $n$ Bernoulli trials is modeled as $Binomial(\textrm{size}=n, \textrm{probability}=p)$ then the maximum likelihood estimate of $p$ equals $\frac{k}{n}$. Suppose this is repeated $M$ times and $k_1$ successes are observed in $n_1$ Bernoulli trials, $k_2$ successes are observed in $n_2$ Bernoulli trials, and so on through $k_M$ successes in $n_M$ Bernoulli trials. The goal to find the maximum likelihood estimate of $p$ if these are modeled as samples from $Binomial(\textrm{size}=n_1, \textrm{probability}=p)$, $Binomial(\textrm{size}=n_2, \textrm{probability}=p)$, and so on through $Binomial(\textrm{size}=n_M, \textrm{probability}=p)$.

## Q1, part 1

(5 points)

Consider the likelihood $L(k_1,k_2,...k_n)$ function for $\{k_1,k_2,...k_n\}$ as outcomes from $M$ independent binomial distributions $Binomial(\textrm{size}=n_1, \textrm{probability}=p),Binomial(\textrm{size}=n_2, \textrm{probability}=p), ...Binomial(\textrm{size}=n_M, \textrm{probability}=p)$.

A)  Please give the likelihood function $L(k_1,k_2,...k_n)$.

    $\Pi_{k=1}^m {n_i\choose k_i}p^{k_i}(1-p)^{n_i-k_i}$

B)  Please give the log of the likelihood function as a sum of terms of the form $\log\left[{n_i \choose k_i}p^{k_i}(1-p)^{n_i-k_i}\right]$

    $\Sigma_{i=1}^m\left( log{n_i \choose k_i} + k_ilog(p) + (n_i - k_i)log(1-p) \right)$

## Q1, part 2

(5 points)

A)  Please give the derivative with respect to $p$ of $\sum_{i=1}^M\left[\log{n_i \choose k_i}+k_i\log(p)+(n_i-k_i)\log(1-p)\right]$.

    $\frac{d}{dp}\sum_{i=1}^M\left[\log{n_i \choose k_i}+k_i\log(p)+(n_i-k_i)\log(1-p)\right]$=$\sum_{i=1}^M\left[\frac{k_i}{p}-\frac{(n_i-k_i)}{(1-p)}\right]$

B)  Please give the value of $p$ that maximizes $\sum_{i=1}^M\left[\log{n_i \choose k_i}+k_i\log(p)+(n_i-k_i)\log(1-p)\right]$.

    $\sum_{i=1}^M\left[\frac{k_i}{p}-\frac{(n_i-k_i)}{(1-p)}\right]=0$

    $\sum_{i=1}^M\left[\frac{k_i}{p}=\frac{(n_i-k_i)}{(1-p)} \right]$

    $\sum_{i=1}^M\left[\frac{1-p}{p}=\frac{(n_i-k_i)}{(k_i)} \right]$

    $\sum_{i=1}^M\left[\frac{1}{p}=\frac{n_i}{k_i} \right]$

    $\sum_{i=1}^M\left[{p}=\frac{n_i}{k_i} \right]$

    $\frac{\sum_{i=1}^mk_i}{\sum_{i=1}^mn_i}$

## Q1, part 3

(5 points)

If the $M$ samples $\{k_1,k_2,...k_n\}$ from $M$ independent binomial distributions $Binomial(\textrm{size}=n_1, \textrm{probability}=p),Binomial(\textrm{size}=n_2, \textrm{probability}=p), ...Binomial(\textrm{size}=n_M, \textrm{probability}=p)$ are viewed as $\sum_{i=1}^Mk_i$ successes in $\sum_{i=1}^Mn_i$ independent Bernoulli trials with probability of success equal to $p$, what is the maximum likelihood estimate of $p$.

$p =\frac{\sum_{i=1}^mk_i}{\sum_{i=1}^mn_i}$

Question 2

## Context

The code below generates a sample, `samp1`, of size 10,000 from the $Binomial(\textrm{size}=20, \textrm{probability}=0.5)$ distribution and a sample, `samp2` of size 10,000 from the $Binomial(\textrm{size}=50, \textrm{probability}=0.3)$ distribution.

```{r}
set.seed(12345)
samp1<-rbinom(10000,20,.5)
dat1<-data.frame(x=samp1)
samp2<-rbinom(10000,50,.25)
dat2<-data.frame(x=samp2)
```

## Q2, part 1

(5 points)

Please display separate histograms of `samp1` and `samp2` with binwidth equal to 1. (your code and plots here)

```{r}
hist1 = ggplot(dat1, aes(x=x)) +  geom_histogram(aes(y = after_stat(density)),binwidth = 1) + theme_classic() + labs(title = "Samp1")
hist1

hist2 = ggplot(dat2, aes(x=x)) +  geom_histogram(aes(y = after_stat(density)),binwidth = 1) + theme_classic() + labs(title = "Samp2")
hist2


```

## Q2, part 2

(5 points)

Treating `samp1` and `samp2` as samples from Normal distributions $Normal(\mu_1,\sigma_1^2)$ and $Normal(\mu_2,\sigma_2^2)$, please give maximum likelihood estimates of $\mu_1$, $\sigma_1^2$, $\mu_2$, and $\sigma_2^2$.

MLE for a normal distribution is $\mu$ and $\sigma$.

```{r}
mu1 = colMeans(dat1)
sigma1 = var(dat1)
str_glue("The mean for mu1 is {mu1} and the standard deviation is {sigma1}")
mu2 = colMeans(dat2)
sigma2 = var(dat2)
str_glue("The mean for mu2 is {mu2} and the standard deviation is {sigma2}")
```

## Answer Q2P2

## Q2, part 3

(5 points)

The plotting methods from `continuous_probability_distributions_2_4_2.Rmd` and practice problem set 2 may be useful here.

A)  For `samp1` please display the density histogram with density curve for $Normal(\mu_1,\sigma_1^2)$ superimposed.

    ```{r}
    g<- ggplot(dat1, aes(x=samp1)) +
            geom_histogram(aes(y = ..density..),binwidth=1) +
            stat_function(fun = dnorm, colour = "red",args = list(mean =mu1, sd = sqrt(sigma1)))
    g
    ```

B)  For `samp2` please display the density histogram with density curve for $Normal(\mu_2,\sigma_2^2)$ superimposed.

```{r}
g<- ggplot(dat2, aes(x=samp2)) +
        geom_histogram(aes(y = ..density..),binwidth=1) +
        stat_function(fun = dnorm, colour = "red",args = list(mean =mu2, sd = sqrt(sigma2)))
g
```

# Question 3

Please carry out the analysis below and answer the questions that follow. For this assignment, please do all calculations in R and show the code and the results in the knit document. Some calculations that may be useful are shown. You are not required to use these.

## Context

In statistical analyses, there are some distributions that aren't Normal, but that can be well-approximated by Normal distributions. From the plots above, some binomial models appear to be good candidates.

To examine this, we will view plots of binomial densities and corresponding Normal distributions for a collection of binomial distributions. We will also compute a measure of the error of approximating the binomial distribution by the corresponding Normal distribution.

Above, we found an appropriate Normal distribution for a given binomial distribution by using maximum likelihood estimates of $\mu$ and $\sigma^2$ based on a sample from a binomial distribution. Here, we calculate the values of $\mu$ and $\sigma^2$ corresponding to the given binomial distribution under the assumption that the proportion of each outcome exactly equals that given by the density function for the binomial distribution.

Suppose the binomial distribution is $Binomial(n,p)$. The maximum likelihood estimate $\bar{x}=\frac{1}{M}\sum_{i=1}^{M}x_i$ for $\mu$ based on a sample $(x_1,...x_M)$ is replaced by $\sum_{k=0}^nkf(k)=\tilde{\mu}$ where $f$ is the density function for $Binomial(n,p)$. To see that this is reasonable, rewrite $\frac{1}{M}\sum_{i=1}^{M}x_i$ as $\sum_{k=0}^{n}\left(\frac{1}{M}\sum_{x_i=k}x_i\right)$. For large $M$, the value $k$ will occur approximately $f(k)M$ times in the sample $(x_1,...x_M)$, so the term $\frac{1}{M}\sum_{x_i=k}x_i$ is approximately $\frac{1}{M}kf(k)M=kf(k)$.

Similarly, The maximum likelihood estimate $\frac{1}{M}\sum_{i=1}^{M}(x_i-\bar{x})^2$ for $\sigma^2$ based on a sample $(x_1,...x_M)$ is replaced by $\sum_{k=0}^n(k-\tilde{\mu})^2f(k)$.

The definition of a function to find the values of the parameters "mean" and "sd" for the Normal distribution corresponding to a binomial distribution with parameters "size"="sz" and "prob"="p" follows:

```{r}
normal.parameters.get<-function(sz,p){
  mu<-sum(0:sz*dbinom(0:sz,sz,p))# weighted average of the binomial outcomes
  sigma.sq<-sum((0:sz-mu)^2*dbinom(0:sz,sz,p))
  return(c(mu,sqrt(sigma.sq)))
}
```

The measure of the error used is the sum of the squared differences between the probability of an outcome $k$ under the binomial distribution and the probability of a value in $(k-0.5,k+0.5)$ under the corresponding Normal distribution.

The definition follows of a function to find this sum of squared differences in probability for a binomial distribution with size parameter equal to "sz" and probability parameter equal to "pr" and the probabilities according to the corresponding Normal distribution:

```{r}
# Function to calculate a sum of square errors is estimating
# a binomial distribution with parameters sz and pr
# by a Normal distribution with parameters norm=(mean,sd) as computed by 
# normal.parameters.get

q1.approx<-function(sz.this,pr.this){
  normal.params<-normal.parameters.get(sz.this,pr.this)
  m<- normal.params[1] # mu
  s<-normal.params[2] # sigma
  # 0.5 below the binomial outcomes {0,1,...sz}
  lower<-0:sz.this-.5
  # 0.5 above the binomial outcomes {0,1,...sz}
  upper<-0:sz.this+.5
  # Probabilities under the Normal distribution
  # for the events [k-.5,k+.5] for k in {0,1,...sz} 
  normal.probs<-pnorm(upper,m,s)-pnorm(lower,m,s)
  # Sum of the squares of the differences between 
  # the probability of the event [k-.5,k+.5] under the Normal 
  # distribution and the probability of k under the binomial
  # distribution
  error<-sum((normal.probs-dbinom(0:sz.this,sz.this,pr.this))^2)
  return(error)
}
```

For example, apply these to the performance of the Normal approximation for $Binomial(148,0.4994)$. These parameters were chosen based on the number of paralytic polio cases and the proportion of vaccinated children from the randomized control trial in the polio data.

```{r}
data("PolioTrials")
dat<-PolioTrials
sz<-sum(dat$Paralytic[1:2])
prop<-dat$Population[1]/sum(dat$Population[1:2])
normal.params<-normal.parameters.get(sz,prop)
mu<-normal.params[1]
sigma<-normal.params[2]

# Plot only values of k that are at all likely.
k<-qbinom(.00005,sz,prop):qbinom(.99995,sz,prop)

# Make a data frame of these values of k, their probability under the binomial distribution, and under the estimated best Normal distribution.
dat.est<-data.frame(k=k,bin.prob=dbinom(k,sz,prop),
            norm.prob=pnorm((k)+0.5,mu,sigma)-pnorm((k)-0.5,mu,sigma))
# Make a column plot for the binomial probabilities showing the Normal approximation.

ggplot(dat.est, aes(x=k))+geom_col(aes(y=bin.prob))+
  geom_point(aes(y=norm.prob))+
  stat_function(fun=dnorm,args = list(mean =mu, sd = sigma))+
  labs(title="Binomial(148,0.4994)")
```

The sum of squared differences in probabilities of the outcomes under the binomial model and under the Normal approximation:

```{r}
q1.approx(sz,prop)

```

## Q3, part 1

(5 points)

Please provide the corresponding visualization $Binomial(10,0.5)$. What is the square error in approximating $Binomial(10,0.5)$ by a Normal distribution in this way?

```{r}
sz = 10
prop = .5

params = normal.parameters.get(sz,prop)
mu = params[1]
sigma = params[2]

k = qbinom(.0005,sz,prop):qbinom(.9995,sz,prop)

dat.est<-data.frame(k=k,bin.prob=dbinom(k,sz,prop),
            norm.prob=pnorm((k)+0.5,mu,sigma)-pnorm((k)-0.5,mu,sigma))

ggplot(dat.est, aes(x=k))+geom_col(aes(y=bin.prob))+
  geom_point(aes(y=norm.prob))+
  stat_function(fun=dnorm,args = list(mean =mu, sd = sigma))+
  labs(title="Binomial(10,0.5)")

str_glue("The square error is {q1.approx(sz,prop)}")
```

## Q3, part 2

(5 points)

What is the square error in approximating Binomial(10,0.1) by a Normal distribution in this way? Binomial(10,0.9)?

```{r}
sz = 10
prop = .1

params = normal.parameters.get(sz,prop)
mu = params[1]
sigma = params[2]

k = qbinom(.0005,sz,prop):qbinom(.9995,sz,prop)

dat.est<-data.frame(k=k,bin.prob=dbinom(k,sz,prop),
            norm.prob=pnorm((k)+0.5,mu,sigma)-pnorm((k)-0.5,mu,sigma))

ggplot(dat.est, aes(x=k))+geom_col(aes(y=bin.prob))+
  geom_point(aes(y=norm.prob))+
  stat_function(fun=dnorm,args = list(mean =mu, sd = sigma))+
  labs(title="Binomial(10,0.1)")

str_glue("The square error for approximating Binomial(10,.1) is {q1.approx(sz,prop)}")

sz = 10
prop = .9

params = normal.parameters.get(sz,prop)
mu = params[1]
sigma = params[2]

k = qbinom(.0005,sz,prop):qbinom(.9995,sz,prop)

dat.est<-data.frame(k=k,bin.prob=dbinom(k,sz,prop),
            norm.prob=pnorm((k)+0.5,mu,sigma)-pnorm((k)-0.5,mu,sigma))

ggplot(dat.est, aes(x=k))+geom_col(aes(y=bin.prob))+
  geom_point(aes(y=norm.prob))+
  stat_function(fun=dnorm,args = list(mean =mu, sd = sigma))+
  labs(title="Binomial(10,0.9)")
str_glue("The square error for approximating Binomial(10,.9) is {q1.approx(sz,prop)}")
```

## Q3, part 3

(5 points)

What is the square error in approximating Binomial(100,0.5) by a Normal distribution in this way? Binomial(100,0.1)? Binomial(100,0.9)?

```{r}
sz = 100
prop = .5

params = normal.parameters.get(sz,prop)
mu = params[1]
sigma = params[2]

k = qbinom(.0005,sz,prop):qbinom(.9995,sz,prop)

dat.est<-data.frame(k=k,bin.prob=dbinom(k,sz,prop),
            norm.prob=pnorm((k)+0.5,mu,sigma)-pnorm((k)-0.5,mu,sigma))

ggplot(dat.est, aes(x=k))+geom_col(aes(y=bin.prob))+
  geom_point(aes(y=norm.prob))+
  stat_function(fun=dnorm,args = list(mean =mu, sd = sigma))+
  labs(title="Binomial(100,0.5)")

str_glue("The square error for approximating Binomial(100,.5) is {q1.approx(sz,prop)}")

sz = 100
prop = .1

params = normal.parameters.get(sz,prop)
mu = params[1]
sigma = params[2]

k = qbinom(.0005,sz,prop):qbinom(.9995,sz,prop)

dat.est<-data.frame(k=k,bin.prob=dbinom(k,sz,prop),
            norm.prob=pnorm((k)+0.5,mu,sigma)-pnorm((k)-0.5,mu,sigma))

ggplot(dat.est, aes(x=k))+geom_col(aes(y=bin.prob))+
  geom_point(aes(y=norm.prob))+
  stat_function(fun=dnorm,args = list(mean =mu, sd = sigma))+
  labs(title="Binomial(100,0.1)")

str_glue("The square error for approximating Binomial(100,.1) is {q1.approx(sz,prop)}")

sz = 100
prop = .9

params = normal.parameters.get(sz,prop)
mu = params[1]
sigma = params[2]

k = qbinom(.0005,sz,prop):qbinom(.9995,sz,prop)

dat.est<-data.frame(k=k,bin.prob=dbinom(k,sz,prop),
            norm.prob=pnorm((k)+0.5,mu,sigma)-pnorm((k)-0.5,mu,sigma))

ggplot(dat.est, aes(x=k))+geom_col(aes(y=bin.prob))+
  geom_point(aes(y=norm.prob))+
  stat_function(fun=dnorm,args = list(mean =mu, sd = sigma))+
  labs(title="Binomial(100,0.9)")

str_glue("The square error for approximating Binomial(100,.9) is {q1.approx(sz,prop)}")
```

## Q3, part 4

(5 points)

For a fixed value of $size$, what condition on the $probability$ of $Binomial(size,probability)$ distribution makes the Normal approximation a better approximation?

Having probability closer to .5 gives us a better approximation

For a fixed value of $probability$, what conditions on the $size$ of $Binomial(size,probability)$ distribution makes the Normal approximation a better approximation? Please feel free to examine the results of values for the size and probability beyond those considered above.

The greater the size the better the approximation

What can you say about the size of the error when the value of $size$ is large and the value of $probability$ is near 0.5?

We can say that the error will be small.
